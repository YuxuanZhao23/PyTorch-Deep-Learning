{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from re import L\n",
    "from torch import nn\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_data(w, b, num_examples):\n",
    "  X = torch.normal(0, 1, (num_examples, len(w))) # 定义均值为0，方差为1，大小为样本数 x 每个样本的（这里可以是随机选）\n",
    "  y = torch.matmul(X, w) + b\n",
    "  y += torch.normal(0, 0.01, y.shape) # 加入噪音均值为0，方差为0.01（噪音一般用正态分布）\n",
    "  return X, y.reshape((-1, 1)) # 把 y 变成列向量\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = torch.tensor(4.2)\n",
    "features, labels = create_synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "  # 这里的 * 的作用是把 (features, labels) 的括号拆掉\n",
    "  dataset = data.TensorDataset(*data_arrays)\n",
    "  return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 100\n",
    "data_iter = load_array((features, labels), batch_size) # batch size 其实比较小是一件好事，因为越小噪音越多，反而避免一开始越走越歪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(2, 1))\n",
    "net[0].weight.data.normal_(0, 0.01) # 初始化 w 和 b\n",
    "net[0].bias.data.fill_(0)\n",
    "loss = nn.MSELoss()\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.194035\n",
      "epoch 2, loss 0.053535\n",
      "epoch 3, loss 0.014853\n",
      "epoch 4, loss 0.004195\n",
      "epoch 5, loss 0.001240\n",
      "epoch 6, loss 0.000418\n",
      "epoch 7, loss 0.000188\n",
      "epoch 8, loss 0.000124\n",
      "epoch 9, loss 0.000106\n",
      "epoch 10, loss 0.000101\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "  for X, y in data_iter:\n",
    "    l = loss(net(X), y)\n",
    "    trainer.zero_grad()\n",
    "    l.backward()\n",
    "    trainer.step()\n",
    "\n",
    "  l = loss(net(features), labels)\n",
    "  print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能用牛顿法吗（二阶导）？不一定能求得到，就算可以，也要一个 nxn 的空间和计算，并不会很容易（一般只能做近似）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
