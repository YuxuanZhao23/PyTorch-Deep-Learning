语言模型助手有三个目标：honest, helpful, harmless

找了很多人去标注哪些是有害的回答，哪些是无害的

helful 和 harmless 往往是对立的，因为什么都说不知道就能提高 harmless，同时也没有用了。从游戏 ELO 的情况来看，harmless 和 helpful 确实是对立的

做 alignment 会不会损害已经拥有的能力，实验显示对于比较大的模型来说，做了RLHF不会损失性能，相反会提升性能。alignment 本质上是让模型更讨人喜欢

在对话中，让人类去标注机器的发言的好坏，这些任务是尽可能的直观和熟悉的。更多是依赖人的直觉，但这种直觉是因人而异的，所以很难去评判标注人员的水平和结果好坏