{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53i352l5FneY"
   },
   "source": [
    "# 引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5775,
     "status": "ok",
     "timestamp": 1733109154853,
     "user": {
      "displayName": "Yuxuan",
      "userId": "14016797690582325984"
     },
     "user_tz": 300
    },
    "id": "3c3HrJa-ukvc"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from re import L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PuohmUQ1vFZ"
   },
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1733112795433,
     "user": {
      "displayName": "Yuxuan",
      "userId": "14016797690582325984"
     },
     "user_tz": 300
    },
    "id": "CPT9JRPGxtYZ"
   },
   "outputs": [],
   "source": [
    "def create_synthetic_data(w, b, num_examples):\n",
    "  X = torch.normal(0, 1, (num_examples, len(w))) # 定义均值为0，方差为1，大小为样本数 x 每个样本的（这里可以是随机选）\n",
    "  y = torch.matmul(X, w) + b\n",
    "  y += torch.normal(0, 0.01, y.shape) # 加入噪音均值为0，方差为0.01（噪音一般用正态分布）\n",
    "  return X, y.reshape((-1, 2)) # 把 y 变成列向量\n",
    "\n",
    "true_w = torch.tensor([[2, -3.4], [5, 1], [7, 9]])\n",
    "true_b = torch.tensor([4.2, 3])\n",
    "features, labels = create_synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "  num_examples = len(features)\n",
    "  indices = list(range(num_examples))\n",
    "  random.shuffle(indices) # 随机打乱的序号\n",
    "\n",
    "  for i in range(0, num_examples, batch_size):\n",
    "    batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)]) # 这里用 min 来处理如果最后一个 batch 长度不足的 edge case\n",
    "\n",
    "    # tensor([features[5], features[3], features[0], features[11]])\n",
    "    yield features[batch_indices], labels[batch_indices] # 每次提供一个 batch 的随机序号的 feature 和对应的 label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1733112830549,
     "user": {
      "displayName": "Yuxuan",
      "userId": "14016797690582325984"
     },
     "user_tz": 300
    },
    "id": "Vk54vjUi3J8V",
    "outputId": "dfe7f8b5-3fda-46d0-fc4f-aa3b6fb727f0"
   },
   "outputs": [],
   "source": [
    "\n",
    "def linreg(X, w, b):\n",
    "  return torch.matmul(X, w) + b\n",
    "\n",
    "def mean_squared_loss(y_hat, y):\n",
    "  return ((y_hat - y) ** 2 / 2).mean()\n",
    "\n",
    "def sgd(params, lr):\n",
    "  with torch.no_grad(): # 暂时关闭 autograd 检查，从而使用 in-place 更改\n",
    "    for param in params:\n",
    "      param -= (lr * param.grad) # 这样写才是 in-place，如果直接用 param = param - (lr * param.grad) 会创建新的 param 从而丢失 grad\n",
    "      param.grad.zero_()\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(3, 2), requires_grad=True) # 初始化成噪音均值为0，方差为0.01，定义可导，所以backward的时候更新的是这个\n",
    "b = torch.zeros(2, requires_grad=True) # b 其实是一个标量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.243436\n",
      "epoch 2, loss 0.101525\n",
      "epoch 3, loss 0.004832\n",
      "epoch 4, loss 0.000282\n",
      "epoch 5, loss 0.000061\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 5\n",
    "net = linreg\n",
    "loss = mean_squared_loss\n",
    "\n",
    "for epoch in range(num_epochs): # 要在数据上跑多少次\n",
    "  for X, y in data_iter(batch_size, features, labels):\n",
    "    l = loss(net(X, w, b), y) # 正着算 loss\n",
    "    l.backward() # l是长度为1的tensor，会更新w\n",
    "    sgd([w, b], lr)\n",
    "  train_l = loss(net(features, w, b), labels)\n",
    "  print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOcRiVmZIBV3QwQcg/mNFx0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "alfred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
